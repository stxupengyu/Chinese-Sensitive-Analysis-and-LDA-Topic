{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LDA_sensitive.ipynb","provenance":[],"collapsed_sections":["dZ3ZnpW4keMT","ZTb9IPoreRSu","kTpk994H_m6C","0UjoB8FUJjiu","f7Df9pzX_yC7"],"authorship_tag":"ABX9TyNZn6SLpyi3A8vZIM1aEn2+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dZ3ZnpW4keMT"},"source":["##Load Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr2b7mnqCzxr","executionInfo":{"status":"ok","timestamp":1611183120126,"user_tz":-480,"elapsed":1013,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"cd9c0b7d-dee2-4ecc-9773-925aabe16b74"},"source":["#connect google drive\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sY3AqyrVDqUK","executionInfo":{"status":"ok","timestamp":1611183120631,"user_tz":-480,"elapsed":1512,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"117a7a9b-4202-49ec-85ec-cd8ad6fbb52f"},"source":["#access current folder\r\n","!ls\r\n","import os\r\n","os.chdir('/content/drive/My Drive/Project/情感分析')\r\n","!ls\r\n","#!git clone https://github.com/AimeeLee77/keyword_extraction"],"execution_count":14,"outputs":[{"output_type":"stream","text":["LDA_sensitive.ipynb  微博数据.xls   情感词汇本体.xlsx\n","stopWord.txt\t     情感词库.xlsx\n","LDA_sensitive.ipynb  微博数据.xls   情感词汇本体.xlsx\n","stopWord.txt\t     情感词库.xlsx\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZTb9IPoreRSu"},"source":["##sensitive corpus"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLs-VZbdYlEi","executionInfo":{"status":"ok","timestamp":1611183126998,"user_tz":-480,"elapsed":7873,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"140a7a3d-b4ca-4c7c-c15f-e94ee355f73f"},"source":["import pandas as pd\r\n","import jieba\r\n","import time\r\n","\r\n","def sensitive_corpus():\r\n","  '''\r\n","  生成positive和negative的词库\r\n","  '''\r\n","  df = pd.read_excel('情感词汇本体.xlsx')\r\n","  print(df.head(2))\r\n","  Happy = []\r\n","  Good = []\r\n","  Surprise = []\r\n","  Anger = []\r\n","  Sad = []\r\n","  Fear = []\r\n","  Disgust = []\r\n","  for idx, row in df.iterrows():\r\n","      if row['情感分类'] in ['PA', 'PE']:\r\n","          Happy.append(row['词语'])\r\n","      if row['情感分类'] in ['PD', 'PH', 'PG', 'PB', 'PK']:\r\n","          Good.append(row['词语']) \r\n","      if row['情感分类'] in ['PC']:\r\n","          Surprise.append(row['词语'])     \r\n","      if row['情感分类'] in ['NA']:\r\n","          Anger.append(row['词语'])    \r\n","      if row['情感分类'] in ['NB', 'NJ', 'NH', 'PF']:\r\n","          Sad.append(row['词语'])\r\n","      if row['情感分类'] in ['NI', 'NC', 'NG']:\r\n","          Fear.append(row['词语'])\r\n","      if row['情感分类'] in ['NE', 'ND', 'NN', 'NK', 'NL']:\r\n","          Disgust.append(row['词语'])\r\n","  Positive = Happy + Good +Surprise\r\n","  Negative = Anger + Sad + Fear + Disgust\r\n","  print('情绪词语列表整理完成')   \r\n","  return Positive, Negative\r\n","\r\n","def emotion_caculate(text):\r\n","  '''\r\n","  计算每一段文本的情感分数\r\n","  '''\r\n","  positive = 0\r\n","  negative = 0\r\n","  wordlist = jieba.lcut(text)\r\n","  wordset = set(wordlist)\r\n","  wordfreq = []\r\n","  for word in wordset:\r\n","      freq = wordlist.count(word)\r\n","      if word in Positive:\r\n","          positive+=freq\r\n","      if word in Negative:\r\n","          negative+=freq\r\n","  emotion_info = {\r\n","      'positive': positive,\r\n","      'negative': negative,\r\n","  }\r\n","  indexs = ['positive', 'negative']\r\n","  return pd.Series(emotion_info, index=indexs)\r\n","\r\n","Positive, Negative = sensitive_corpus()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["   词语 词性种类  词义数  词义序号 情感分类  强度  极性 辅助情感分类  强度.1  极性.1 Unnamed: 10  Unnamed: 11\n","0  脏乱  adj  1.0   1.0   NN   7   2    NaN   NaN   NaN         NaN          NaN\n","1  糟报  adj  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN          NaN\n","情绪词语列表整理完成\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kTpk994H_m6C"},"source":["##sensitive analysis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzaacozQfb11","executionInfo":{"status":"ok","timestamp":1611183141738,"user_tz":-480,"elapsed":22609,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"d98b9ca5-a930-49de-b452-fb436c1ed849"},"source":["!pip3 install pandarallel\r\n","import numpy as np\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","from pandarallel import pandarallel\r\n","\r\n","def data_pp():\r\n","  '''\r\n","  为每条文本赋予对应的情绪分数\r\n","  '''\r\n","  weibo = pd.read_excel('微博数据.xls',sheet_name = 1).astype(str).iloc[:,2:3]#sheet name 为不同的表\r\n","  weibo = weibo.dropna()\r\n","  print(weibo.head())\r\n","  #并行初始化\r\n","  pandarallel.initialize()\r\n","  start = time.time()   \r\n","  emotion_df = weibo['正文'].parallel_apply(emotion_caculate)\r\n","  #emotion_df = weibo_df['review'].apply(emotion_caculate)\r\n","  end = time.time()\r\n","  print(end-start)\r\n","  print(emotion_df.head())\r\n","  #将情绪分数转化为二值\r\n","  emotion_df['po>ne'] = [1 if emotion_df['positive'][i]>=emotion_df['negative'][i] else 0 for i in range(len(emotion_df))]\r\n","  emotion_df.head()\r\n","  #将文本与对应的情感合并\r\n","  output_df = pd.concat([weibo, emotion_df['po>ne']], axis=1)\r\n","  output_df.head()\r\n","  return output_df\r\n","\r\n","def split_data(output_df):\r\n","  '''\r\n","  把两种情感分别保存在两个表中\r\n","  '''\r\n","  po = []\r\n","  ne = []\r\n","  for i, value in enumerate(output_df['po>ne']):\r\n","    if value==1:\r\n","      po.append(output_df['正文'][i])\r\n","    else:\r\n","      ne.append(output_df['正文'][i])\r\n","  print('len(po)\\n',len(po))\r\n","  print('len(ne)\\n',len(ne))\r\n","  po = pd.DataFrame(po)\r\n","  ne = pd.DataFrame(ne)\r\n","  print('po.head()\\n',po.head())\r\n","  return po, ne\r\n","  \r\n","output_df = data_pp()\r\n","po, ne = split_data(output_df)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.5.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.3)\n","                                                  正文\n","0  通州早就比首都拥堵了，这下该挤爆了 //@RODGA:然后通州也变北京那样[doge][do...\n","1  北京减负，疏散非首都功能，最该迁出去的就是央企总部。像中外运长航、三峡集团、中国能源建设集团...\n","2  不是疏散北京非首都功能吗？ //【宝沃国产项目落户北京 斥资50亿搞研发】宝沃国产项目落户北...\n","3  【疏解北京】《纲要》明确了京津冀区域的目标定位，即要打造成以首都为核心的世界级城市群。这表明...\n","4  #生态京津冀#“以减法换加法”。近年来，本市非首都功能产业疏散的减法换来园林绿化建设的加法。...\n","INFO: Pandarallel will run on 2 workers.\n","INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n","11.48845100402832\n","   positive  negative\n","0         1         1\n","1         2         1\n","2         0         1\n","3         4         2\n","4         5         0\n","len(po)\n"," 233\n","len(ne)\n"," 19\n","po.head()\n","                                                    0\n","0  通州早就比首都拥堵了，这下该挤爆了 //@RODGA:然后通州也变北京那样[doge][do...\n","1  北京减负，疏散非首都功能，最该迁出去的就是央企总部。像中外运长航、三峡集团、中国能源建设集团...\n","2  【疏解北京】《纲要》明确了京津冀区域的目标定位，即要打造成以首都为核心的世界级城市群。这表明...\n","3  #生态京津冀#“以减法换加法”。近年来，本市非首都功能产业疏散的减法换来园林绿化建设的加法。...\n","4  北京市为了疏散人口，转移非首都功能，立志要把全部或部分行政事业单位迁出，劳民伤财啊？能不能不...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0UjoB8FUJjiu"},"source":["##data pre-processing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCC9XYTgD83R","executionInfo":{"status":"ok","timestamp":1611183146038,"user_tz":-480,"elapsed":26903,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"411ed1fc-0709-4858-94c4-55934a765f08"},"source":["import sys,codecs\r\n","import jieba.posseg\r\n","import jieba.analyse\r\n","\r\n","def dataPrepos(text, stopkey):\r\n","  '''\r\n","  数据预处理操作：分词，去停用词，词性筛选\r\n","  '''\r\n","  l = []\r\n","  pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd']  # 定义选取的词性\r\n","  seg = jieba.posseg.cut(text)  # 分词\r\n","  for i in seg:\r\n","      if i.word not in stopkey and i.flag in pos:  # 去停用词 + 词性筛选\r\n","          l.append(i.word)\r\n","  return l\r\n","\r\n","def data_pp(data, stopkey):\r\n","  '''\r\n","  对每一行文本预处理\r\n","  '''\r\n","  idList, abstractList = data.index, data[0]\r\n","  corpus = [] # 将所有文档输出到一个list中，一行就是一个文档\r\n","  for index in range(len(idList)):\r\n","      text = abstractList[index] # \r\n","      text = dataPrepos(text,stopkey) # 文本预处理\r\n","      #text = \",\".join(text) # 连接成字符串，空格分隔\r\n","      corpus.append(text)\r\n","  print(corpus[1:3])\r\n","  return corpus\r\n","\r\n","stopkey = [w.strip() for w in codecs.open('stopWord.txt', 'r').readlines()]\r\n","data = po#选择positive的情感\r\n","corpus = data_pp(data, stopkey)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[['减负', '疏散', '首都', '功能', '最该', '迁出去', '总部', '集团', '能源建设', '集团', '集团', '石油', '铝业', '必要', '总部', '设在', '迁来', '领导', '经济', '利益', '欢迎', '总部', '加剧', '企业', '总部', '功能', '首都', '功能'], ['疏解', '纲要', '京津冀', '区域', '目标', '定位', '造成', '首都', '核心', '世界级', '城市群', '表明', '低端', '产业', '疏散', '发展', '区域', '辐射', '作用', '高端', '产业', '重点', '疏散', '首都', '功能', '一定', '时间段', '主城区', '人口', '降下来', '重点', '在于', '人口', '重新聚集']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f7Df9pzX_yC7"},"source":["##LDA "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dh3IBhHG_Hvy","executionInfo":{"status":"ok","timestamp":1611183146594,"user_tz":-480,"elapsed":27454,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"8bf0053e-eb50-4886-b650-a7f09b505707"},"source":["from gensim import corpora,models\n","import re\n","\n","def lda_result(corpus):\n","  # 主题分析\n","  neg_dict = corpora.Dictionary(corpus)\n","  neg_corpus = [neg_dict.doc2bow(i) for i in corpus]\n","  neg_lda = models.LdaModel(neg_corpus,num_topics = 3,id2word = neg_dict)\n","  neg_theme = neg_lda.show_topics()\n","  # print(neg_theme)#展示主题\n","  pattern = re.compile(r'[\\u4e00-\\u9fa5]+')\n","  print(pattern.findall(neg_theme[0][1]))# 取出高频词\n","  pos_key_words =[]\n","  for i in range(3):\n","    pos_key_words.append(pattern.findall(neg_theme[i][1]))\n","  pos_key_words = pd.DataFrame(data=pos_key_words,index=['主题1','主题2','主题3'])\n","  print(pos_key_words)  \n","  return pos_key_words\n","\n","record_topic = lda_result(corpus)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["['投资', '首都', '发展', '功能', '疏散', '产业', '经济', '物流', '国际', '京津冀']\n","      0   1   2   3   4   5   6   7   8    9\n","主题1  投资  首都  发展  功能  疏散  产业  经济  物流  国际  京津冀\n","主题2  功能  首都  疏散  投资  雄安  发展  产业  企业  经济   物流\n","主题3  投资  发展  产业  功能  经济   雾  首都  疏散  企业   大哥\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0bX_Ggg7_Q8S","executionInfo":{"status":"ok","timestamp":1611183146595,"user_tz":-480,"elapsed":27453,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}}},"source":[""],"execution_count":18,"outputs":[]}]}